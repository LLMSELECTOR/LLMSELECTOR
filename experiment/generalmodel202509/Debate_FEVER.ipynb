{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d93d4cc6-3305-4219-9067-fab3025945ac",
   "metadata": {},
   "source": [
    "## Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f25114-34d7-414e-930b-163c0ac6dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmselector.data_utils.fever import DataLoader_FEVER\n",
    "from llmselector.compoundai.module.debate import MultiAgentDebateMultiRound\n",
    "from llmselector.compoundai.metric import Metric, compute_score\n",
    "from llmselector.compoundai.optimizer import OptimizerFullSearch, OptimizerLLMDiagnoser\n",
    "import llmselector, os\n",
    "import os\n",
    "db_path = '../../cache/db_fever.sqlite'\n",
    "if not os.path.exists(db_path): \n",
    "    !wget -P ../cache https://github.com/LLMSELECTOR/LLMSELECTOR/releases/download/0.0.1/db_fever.sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b57b2-dd4a-4425-886f-87fd9b0cae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmselector.config.config(\n",
    "    db_path=db_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50409a61-078e-46b6-9fcf-0f91ee16763b",
   "metadata": {},
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb5e552-d563-4734-b0f2-c8009f559b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Mydataloader = DataLoader_FEVER()\n",
    "q_data = Mydataloader.get_query_df()\n",
    "#q_data = q_data.drop(q_data.index[[1588,2272]]) # these points invoke the output filtering by claude\n",
    "random_state = 2025\n",
    "random_state_list = [2027,2028,2029,2025,2026]\n",
    "opt_seed_list = [0,0,0,0,0]\n",
    "num_worker=40\n",
    "train_df, test_df = train_test_split(q_data,test_size=0.5, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f3f8da-d0e2-4159-aed6-1f8eb580ec09",
   "metadata": {},
   "source": [
    "## 2. Specify model and eval metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e5af1-2e91-4eaa-8ddb-10899c7ea465",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = ['gpt-5-2025-08-07minimal','gpt-5-mini-2025-08-07minimal','gpt-5-nano-2025-08-07minimal',\n",
    "              'claude-opus-4-1-20250805minimal','claude-sonnet-4-20250514minimal',#'claude-3-5-haiku-20241022',\n",
    "              'gemini-2.5-prominimal','gemini-2.5-flashminimal','gemini-2.5-flash-liteminimal',\n",
    "              ]\n",
    "M1 = Metric('em_direct')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f410760-5e66-48f1-9409-98a494029830",
   "metadata": {},
   "source": [
    "## 3. Standard systems using one fixed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47288dd5-46cc-4852-b389-ab510a18fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Agents_SameModel ={}\n",
    "num_debator=3\n",
    "round=2\n",
    "prompt_template_debate='''Below is a user question, your own answer, and other agents' answers. Can you please update your answer? Critically analyze your solution, that of the other agents, as well as your own knowledge. Then give your final answer at the end as (X), where X is one of SUPPORTS, REFUTES, and NOT ENOUGH INFO.\n",
    "[User Question]:{query}\n",
    "[Your Answer]: {response}\n",
    "[Other agents' answers]: {other_responses}\n",
    "'''\n",
    "prompt_template_initdebate='''Verify the following statement accurately. Give your answer as (X), where X is one of SUPPORTS, REFUTES, and NOT ENOUGH INFO. Give a one-sentence explanation.\n",
    "[Claim]: {query}\n",
    "'''\n",
    "for name in model_list:\n",
    "    Agents_SameModel[name] = MultiAgentDebateMultiRound(num_debator=num_debator,round=round,\n",
    "                                                       prompt_template_initdebate=prompt_template_initdebate,\n",
    "                                                        prompt_template_debate=prompt_template_debate,\n",
    "                                                         )\n",
    "    Opt0 = OptimizerFullSearch(model_list = [name])\n",
    "    Opt0.optimize(train_df, M1, Agents_SameModel[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7f2e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_systems = {**Agents_SameModel}\n",
    "results_train = compute_score(All_systems, train_df, M1)\n",
    "display(\"train accuracy\",results_train)\n",
    "results = compute_score(All_systems, test_df, M1)\n",
    "display(\"test accuracy\",results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddc2ddc-96a2-4fba-ac8f-250d2933bec4",
   "metadata": {},
   "source": [
    "## 4. LLMSELECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a157ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(q_data, random_seed=2025, opt_seed=0, log_path=r'../../log/TableArithmetic/',train_size=500):\n",
    "    train_df, test_df = train_test_split(q_data,test_size=0.5, random_state=random_seed)\n",
    "    LLMSELECTOR = MultiAgentDebateMultiRound(num_debator=num_debator,round=round,\n",
    "                                                       prompt_template_initdebate=prompt_template_initdebate,\n",
    "                                                        prompt_template_debate=prompt_template_debate,\n",
    "                                                         )\n",
    "    Optimizer = OptimizerLLMDiagnoser(model_list = model_list,max_budget=1000,max_worker=num_worker,seed=opt_seed)\n",
    "    score_hist = Optimizer.optimize( train_df.head(train_size), M1, LLMSELECTOR,show_progress=False)\n",
    "    All_systems = {\"LLMSELECTOR\": LLMSELECTOR, **Agents_SameModel}\n",
    "    results_train = compute_score(All_systems, train_df, M1)\n",
    "    display(\"train accuracy\",results_train)\n",
    "    os.makedirs(log_path, exist_ok=True)\n",
    "    results_train.to_csv(f\"{log_path}/train_acc_{random_seed}_{opt_seed}.csv\")\n",
    "    results = compute_score(All_systems, test_df, M1)\n",
    "    display(\"test accuracy\",results)\n",
    "    results.to_csv(f\"{log_path}/test_acc_{random_seed}_{opt_seed}.csv\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3353a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "dataname = f'MAD_FEVER_generalmodel2025_20250918'\n",
    "[run_experiment(q_data, random_seed=x, opt_seed=y, log_path=f'../../log/{dataname}/') for x,y in tqdm(zip(random_state_list,opt_seed_list))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMSELECTOR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19 (main, May  6 2024, 14:46:57) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a6d27e8a51c009f5716a7500b3d00c6679297ab34466bc881319ad79ecd6a0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
